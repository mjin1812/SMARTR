---
title: "Denny Lab Wholebrain Pipeline Demo"
author: ' by Michelle Jin'
date: "Septermber 30, 2021"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
  word_document: default
---

<div style="line-height: 2em;">
<font size="4"> 
<style type="text/css">

h1.title {
  font-size: 38px;
  color: Black;
  text-align: center;
}
h4.author { /* Header 4 - and the author and data headers use this too  */
  font-size: 18px;
  color: Black;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 18px;
  color: Black;
  text-align: center;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

# {.tabset .tabset-fade .tabset-pills}

# 0 Pre-processing

Prior to using this analysis pipeline, the data fed into the pipeline needs to be formatted in the appropriate format. This section will describe the process of generating automatic segmentation data and creating a MAX projection image from the raw `.lif` file obtained from the 
SP8 microscope. The details of the code underlying the segmentation scripts are beyond the scope of this tutorial. 


![*Pre-processing workflow for the Sundowning project*](C:\Users\Jinmich\Documents\SMARTR\tutorial\Pipeline_preprocessing.png)



The schematic above is a color-coded illustration of the pre-processing steps for segmentation and registration data prior to feeding the data into R. Anything in round brackets () is indicative of the particular data file, such as a .txt or .tif that is generated at that step. Anything in square brackets [] is indicative of an ImageJ macro used to generate the files. For now, these macros can be found at `/Michelle Jin/WholebrainSegmentationScripts/Local` on Denny Lab Server V.

Note that some of these scripts have underlying assumptions about the imaging data. For example, `batch_create_tiff_sundowning.ijm` assumes that the `.lif` file from the microscope has the following channel orders:

1) If there are two channels then C1 =  eyfp & C2 = cfos
2) If there are 3 channels, then the first channel is erroneous or empty and C2 = eyfp while C3 = cfos

This is due to the specific organization conventions of the Sundowning images. Some of the scripts may need modification if you would like to adapt them for just one channel, e.g. cfos. 


## 0.1 Auto-segmentation

First the `.lif` file needs to be converted to a `.tif` file with the same name. This is just a more standardized format without proprietary association with Leica microscopes. This makes point B a good entry point for external labs that may want to use this pipeline in the future. 

This `.tif` file is then run through an ImageJ macros for automatic segmentation of the cfos channel. This macro is recursive and will search through all subdirectories for `tif` files with the same name as the original `lif` file. 

A similar script is run for automatic segmentation of the cfos channel and colocalization detection. The output `.txt` files highlighted in red are the segmentation data that get read by this R package. 




## 0.2 Creating a Max projection

The `MAX_name.tif` file highlighted in blue is used as the reference image during interactive registrations. This is the a flattened version of the .`tif` file from step B. The `wholebrain` package can only do registrations on 2D tiff images so this `MAX_name.tif` is used to calculate the approximate transformation that should happen to all the cells segmented in 3D. Importantly, the brain contours in this `MAX_name.tif` cannot touch any of the 4 borders of an image. These images should either be manually edited in ImageJ to omit brain contours intersecting the image border, or the original `.tif` image (B) should be padded with a small border of black pixels. 


## 0.3 File organization

The imaging data is assumed to be organized with the following folder structure:

`Denny Lab server No. > user folder > project folder > mouse no. > slice no.`


# 1 Overview 

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook that allows for an interactive walkthrough of the wholebrain pipeline for the lab. When you execute code within the notebook, the results will appear beneath the code. To execute a code chunk in this notebook click the *Run* button within a chunk or place your cursor inside it and pressing *Ctrl+Shift+Enter*. 

The base of this pipeline is an R package in development with the working title, `SMARTR`, a self-referential play on a previous package I developed as an extension to `wholebrain` called `SMART`. 

In this notebook, a prototypical example of of the following steps in processing data from a mouse will be demonstrated:

I.    Setting up the pipeline by specifying experimental parameters and save directories.
II.   The interactive registration process.
III.    Importing raw segmentation data from .txt files generated from ImageJ for multiple channels.
VI.   Optionally creating a filter for the 'cfos' and 'eyfp' channels to clean segmented counts. 
V.    Creating a segmentation object that is compatible with `wholebrain` functions.
VI.   Forward warping and mapping the data onto the standardized mouse atlas.
VII.    Cleaning the mapped data in all the following ways:
        A.   Removing cells that map outside the boundaries of the atlas.
        B.   Omitting regions by a default list of regions to omit.
        C.   Omitting regions by user specified region acronyms per hemisphere.
        D.   Removing Layer 1 cells
        E.   Removing cells from a contralateral hemisphere per slice if the registrations are divided by right and left hemispheres. 
VIII.   Obtaining cell counts normalized by region volume (per mm^2^) and region areas (per mm^2^). 
IX.   Splitting the hippocampal cell counts into Dorsal and Ventral based on a user-defined AP coordinate ranges.
X.    Aggregating cell counts across multiple animals.
XI.   Quality checks to look for outliers in region cell counts prior to analysis.  
XI.   Functions for easy analysis, based on categorical variables entered as mouse attributes, e.g. cross correlations and network analysis.
  
  
## 1.1 OOP in R

Before getting started, it's helpful to get a sense of a how the structure of data is handled and bundled together in this pipeline. Data is now stored in S3 data type objects called `slice` and `mouse`. The data in these objects will be manipulated by a special type of function called a generic function (this is analogous to an object method in python). Generic functions allow you to pass objects of different classes to the same function, and it can recognize and perform different operations on objects depending on their class. If this is all confusing to you, don't worry at all! This is much more information than you actually need to know to use this pipeline. It's just helpful to better understand the architecture of the package.  


To get an excellent brief overview of what object oriented programming (OOP) is and it's advantages over procedural programming, check out this excellent 
[YouTube video!](https://www.youtube.com/watch?v=pTB0EiLXUC8&t=379s&ab_channel=ProgrammingwithMosh) 


<br>  


# 2 Installation 

## 2.1 Install `wholebrain`

Installing `wholebrain` is quite finicky. The most updated instructions on installing `wholebrain` for Windows is found here [!]. 

For Macs, the instructions are found on the Denny lab google drive under Protocols > Whole-Brain > Installation


## 2.2 Install `SMARTR`

Currently the pipeline package is on my github, but it is a private respository. Therefore to download it from github using the `devtools` package, we need to feed the `install_github()` function a personal access token to access the github API. There are also some dependencies on the tidyverse packages so we will install that too. Install the package with the code below:

```{r, eval = FALSE}
# Install from CRAN
install.packages("tidyverse")`

# Install ggpattern package (useful package for plotting) 
devtools::install_github("coolbutuseless/ggpattern")

# personal access token
auth_token <- "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"

# Set the devtools install options for just 64-bit architecture
options(devtools.install.args = "--no-multiarch")   

# Install package
devtools::install_github("mjin1812/SMARTR@main", auth_token = auth_token)


```

We can now load the package!

```{r, eval=TRUE, message=FALSE}
# Load SMARTR
library(SMARTR)
```

You can pull up the package description with the code below:

```{r}

?SMARTR

```

> Tip: From now on, get in the habit of using the `?` operator to pull up the help page about a package, object, function, or piece of data

<br>  

# 3 Object descriptions

## 3.1 Slice objects

A `slice` object will contain all the data related to registration, segmentation for each channel, and cell counts for a particular image.

It will also contain "metadata" about your experimental images, such as what the experimenter-assigned slice ID is, which brain atlas AP coordinate matches best with the given image, and what the path to the image used for registration is. These metadata are stored as the object's attributes.

Run the code below to used the `?` operator to pull up the documentation for a `slice` object. This will pull up a help page description of all the `slice` object attributes. 


```{r}
?SMARTR::slice

```

> Tip: Usage of the `::` or double colons here means that R specifically looks for a function, object, or help page in `SMARTR` package. This isn't necessary if you load the package, but it can sometimes help avoid ambiguity if there are identical names for things in other loaded packages.


## 3.2 Mouse objects

A `mouse` object is an object that will store multiple `slice` objects (and therefore all the information in it), and will eventually store the combined cell data and the  region cell counts normalized by volume. Like a `slice`, it will also contain "metadata" about your mouse stored as attributes. Now, try pulling up the help page yourself for the `mouse` object to see all attributes you can store!

<br>  


# 4 Pipeline setup

Okay enough background! Let's walkthrough the pipeline with an example mouse and image to process.


## 4.1 Initializing a mouse object 

Let's create an instance of a mouse object. This mouse object will store data from mouse No. 325, so
we will name it `mouse_325`. We also want to store the important experiment metadata related to this mouse.
All of this can be done using the `SMARTR::mouse()` constructor function:

```{r}
# Create and store information for mouse 325
mouse_325 <- mouse(mouse_ID = "325",
                   sex = "male",
                   strain ="129s",
                   experiment = "sundowning",
                   group = "AD",
                   cohort = "3_2_months",
                   age = 2,
                   output_path = 'V:/Michelle Jin/Sundowning/PipelineDemo/Mouse_example') 


# Let's check the data stored correctly with print()
print(mouse_325)


```

Note that when we don't initially store the mouse metadata using variables passed to the mouse object constructor, this metadata is 'empty' and there are default values stored as placeholders.

```{r}
print(mouse())
```


## 4.2 Modifying mouse attributes

If you find that you've made a mistake in setting the attributes for a mouse object, have no fear! These attributes can be easily modified. We just need to pull out the `info` list containing the mouse attributes and manually correct them:

 
```{r}
# get the mouse info list
mouse_info <- attr(mouse_325, 'info')

# Change mouse attributes to reflect your mouse and experiment.
mouse_info$sex           <- 'female'
mouse_info$group         <- 'control'
mouse_info$cohort        <- "4_2_months"

# Change mouse's attributes by storing the mouse info list back into the mouse
attr(mouse_325, 'info') <- mouse_info

# Check the updates 
print(mouse_325)

```
We have now finished setting up a mouse object and are ready to store some imaging and automated cell count data into it!

## 4.3 Initializing a slice object

Now we need to create a slice object. For demonstration, this slice will be slice No. S1_1, and we also want to store imaging 
metadata as the slice object's attributes. For this we'll use the `SMARTR::slice()` constructor function.

Before doing this, look at your image and compare it with a standardized mouse atlas to decide what the most accurate AP coordinate should be prior to creating the object. 

Additionally, if the brain was slanted when slicing, so that each hemisphere aligns to a different AP coordinate, then you need to initialize a new slice for each hemisphere to process each slice separately. Here I've created a slice including both hemispheres; later we will clean data from a right hemisphere slice and omit the contralateral hemisphere for demonstration.

If the left and right side align well onto a single atlas plate, initialize only one slice object and set the `hemisphere` attribute to NULL (or don't set it, as this is the default).


```{r}
slice_1_1 <- slice(slice_ID = "1_1",  
                   coordinate = 1.0,
                   conversion_factor = 1.0833, # Pixel-to-micron conversion factor
                   bin = 1,                    # If the image was downsampled by a bin factor in imageJ
                   z_width = 20,               # z-stack thickness in microns
                   hemisphere = NULL,          # "left", "right" or NULL (both sides)
                   channels =c('cfos', 'eyfp', 'colabel'),  # Channels to process
                   registration_path = 'V:/Holly Hunsberger/Sundowning_images/Cohort4_2month/325/325_1_1/MAX_325_1_1.tif'
                   )  

# Let's check the data stored correctly with print()
print(slice_1_1)
```

## 4.4 Modifying slice attributes

To fix or modify slice attributes, we apply the same method that we used for mouse objects.

```{r}
# We can access the list used to print the slice attributes with the command below
slice_info <- attr(slice_1_1, 'info')

# Change slice attributes to reflect your slice and experiment
slice_info$coordinate          <- 0.89
slice_info$z_width             <- 24     

# Change slice attributes by storing the slice info list
attr(slice_1_1, 'info') <- slice_info

# Check the updates 
print(slice_1_1)

```
TODO: allow slice attributes channels to be the default in the channels parameters in functions if none is supplied.

TODO: allow for relative paths based on the mouse input directory


## 4.5 Adding slice objects to mouse objects

We are ready to bundle our slice information with our mouse. But first...


Type the code below into the R console: 

```{r}
`mouse_325$`
```

You should see that a named list called `slices` pops up and you can complete the suggestion by hitting TAB.
The `$` operator is very useful for accessing any named element in a list. Right now, the `slices` list is NULL,
because it is empty and doesn't contain anything.

> Tip: You can use the `$` operator to look at named elements in a mouse. 


```{r}
# Check the length of slices in a mouse first
length(mouse_325$slices)

```


That will change soon after we add the slice into our mouse. Let's check out the help page of the function `add_slice()`. 

> Tip: Check the "Usage" or "Examples" section for a code example of how to use a particular function in a package

```{r}

?add_slice

```

Now that we've read how to use the function, let's add our slice to our mouse with the line below:

```{r}
mouse_325 <- add_slice(mouse_325, slice_1_1)

# Check the length of slices now
length(mouse_325$slices)

# Access the new slice information with the code below
mouse_325$slices$`1_1`

```

# 5 Interactive registration

Now we are ready to begin registering this slice! For those of you who are unfamiliar, registration is the process of aligning your imaging dataset with a standardized mouse atlas. `wholebrain` does this by generating a set of correpondence points around the contours of the brain in your image, and aligning it with analagous points around an atlas plate from the standardized mouse atlas. 

Before we register we must first check that the contours of our brain sample can be detected properly. The reason we do this is because
`wholebrain`'s initial guess of correspondence point mappings are often way off with our images. 


## 5.1 Detecting brain contours

To get a good outline of our brain, we need to modify a critical parameter called the `brain.threshold`. The `brain.threshold` is 
just one of many parameters that are stored in a `filter` list. This filter is passed on to various wholebrain functions. Later,
we will pass this filter to`SMARTR::register()` so knows the function knows which `brain.threshold` to use. 

We will adjust and check the effects of changing the `brain.threshold` parameter using the function `adjust_brain_outline()` This function uses a default `brain.threshold` of 10 and pops up a window showing the detected contours in a blue line. If the contours are unsatisfactory, you can use the interactive console interface to modify the value. I recommend modifying the value in steps of +/- 2.

> Note that another GUI window pops up to modify various filter parameters, including the `brain.threshold`. 
You can also try to use this to modify your values, however it is quite buggy and often crashes.  Hopefully one day we find a threshold that universally works and can skip this step!


```{r}
# Pass a slice object as an argument
# Interactively adjust the brain threshold until it looks good
# Store the output as a filter

filter <- adjust_brain_outline(mouse_325$slices$`1_1`)

```
  

## 5.2 Registration of a slice

The `register()` function is one of the generic functions of the package. Because of this, what the function does depends on the type of objects being fed into it. The `register()` function can be used on both `slice` and `mouse` objects. Examples of how to used this function with `slice` or `mouse` objects are found under the Usage section.

Pull up the help page with the code below:

```{r}
?register

```


The 'Usage' and 'Examples' sections will tell you how to use this function depending on if a `slice` or `mouse` object is the argument. The 'Value' section indicates what type of data is returned. Now that we've read up, let register!

```{r, eval = FALSE}
mouse_325 <- register(mouse_325, 
                      slice_ID = "1_1",
                      hemisphere = NULL,
                      filter = filter)

```

> Tip: The `mouse` object may contain many slices, so you need to specify which `slice_ID` and 
which `hemisphere` you want to register


At this point, you may find it useful to save all your hard work after perfecting the registration. You can save to the mouse output folder with the command below.

```{r, eval = FALSE}
save_mouse(mouse_325)
```

Add the `timestamp` parameter to save the mouse object with today's date:

```{r, eval = FALSE}
save_mouse(mouse_325, timestamp = TRUE)
```


I recommend always saving with a timestamp so you never lose more than a day's worth of work if you accidentally overwrite something. 

To save time, we will now load a dataset that has been pre-registered:

```{r}
load("V:/Michelle Jin/Sundowning/PipelineDemo/Mouse_example/mouse_325_pre-registered.RDATA")
```



# 6 Add segmentation data

## 6.1 Import raw ImageJ data

The segmentation data from ImageJ is stored into .txt files. We can use the `import_segmentation()` generic function to import the raw data. 

Note that currently, this method is dependent on the naming conventions of the txt file, and therefore may be specific to Holly's data right now. In the future, if there are multiple different ImageJ segmentation macros with different naming conventions being used in the lab, we just need to write multiple custom import functions. This won't interfere with the rest of the pipeline.

```{r}
mouse_325 <- import_segmentation(mouse_325,
                                 slice_ID = '1_1',
                                 hemisphere = NULL,
                                 channels = c('eyfp', 'cfos', 'colabel'))

```

## 6.2 Filtering channels for false positives (Optional)

If you have gone through the process of identifying the best variables and ranges to filter out false positive counts from true positive counts for your dataset, you can used the `make_segmentation_filter()` to further pre-process your data for the 'cfos' and 'eyfp' channels. This function is flexible, in that it allows you to process either the eyfp channel, cfos channel, or both and allows for setting separate variables and filtering ranges for either channel.


```{r}
# Create a filter of non-cells for the eyfp channel

mouse_325 <- make_segmentation_filter(mouse_325,
                                      slice_ID = '1_1',
                                      hemisphere = NULL,
                                      channels = "eyfp",
                                      params = list(c("Vol..unit.", "Spher..unit.", "Moment1", "Moment2", "Moment3", "Moment4", "Moment5")),
                                      ranges = list(list(c(222, 8350),
                                                        c(0.43, 0.80),
                                                        c(0.45, 50),
                                                        c(0, 458),
                                                        c(0, 1097),
                                                        c(0, 4.4),
                                                        c(0, 0.035)
                                                        ))
                                      )

```


## 6.3 Creating a segmentation object 

We need to create a segmentation object compatible with `wholebrain` functions from the imported raw segmentation data. We'll use the `make_segmentation_object()` generic function. 


```{r, eval = FALSE}
mouse_325 <- make_segmentation_object(mouse_325,
                                      slice_ID = '1_1',
                                      hemisphere = NULL,
                                      channels = c('eyfp', 'cfos', "colabel"),
                                      use_filter = TRUE)

```

> Tip: Set the use_filter parameter to FALSE to avoid using the optional filter


# 7 Mapping cells to atlas space

## 7.1 Forward warp data to atlas space

We are ready to map our segmentation data onto atlas space! We will forward warp our segmented cells onto atlas space with the `map_cells_to_atlas()` generic function. Within this function, the package calls `wholebrain::inspect.registration()` from the base wholebrain package. 


```{r, eval = FALSE}

mouse_325 <- map_cells_to_atlas(mouse_325,
                                slice_ID = '1_1',
                                hemisphere = NULL,
                                channels = c('eyfp', 'cfos', "colabel"),
                                clean =  FALSE,
                                display = TRUE)
```



## 7.2 Cleaning mapped cell data

Now we can clean the data we've mapped. To demonstrate cleaning of a slice with a single hemisphere, I'm going to cheat a little bit and just pre-load slice object `slice_1_2_right` into the global environment. This slice has been pre-registered, segmentation data has been imported, and forward warped.


```{r}
# load slice example
load("V:/Michelle Jin/Sundowning/PipelineDemo/325/example_slice.RDATA")

```


```{r}
# Add the right hemisphere slice object into the mouse object
mouse_325 <- add_slice(mouse_325, slice_1_2_right)

```


Now we have two slice objects in our mouse, one with both hemispheres of data and another slice from which we only want the right hemisphere's data. 

For all slices you analyze, there may be an automatic list of regions you'd like to exclude for each hemisphere. This is automatically set as a slice attribute when you create it and you can edit it like any other slice attribute as demonstrated earlier. In the slice attributes, a list of these regions can be accessed with `$left_regions_excluded` and `$right_regions_excluded`.

When you run the `exclude_anatomy` function, it will automatically omit the regions for each hemisphere in these lists. The commands below print the default excluded regions for the left and right hemispheres.

```{r}
# Print the default regions excluded list for the right hemisphere
print(attr(mouse_325$slices$`1_1`, "info")$right_regions_excluded)

# Print the default regions excluded list for the left hemisphere
print(attr(mouse_325$slices$`1_1`, "info")$left_regions_excluded)

```


For the right hemisphere slice object, we want to: 

* exclude the contralateral hemisphere
* clean up cell counts that map outside of the brain contours
* exclude cell counts from layer 1 of the cortex
* manually specify the right-sided regions we want to exclude

Pull up the help page of `exclude_anatomy` to get the names of the arguments that accomplish these tasks.

In this example right-sided slice, I've specified the additional region VS to exclude both the TRS and LSX (lateral and third ventricles) using the parameter `exclude_right_regions`. Analogously, You would use the `exclude_left_regions` for a slice object that was left-sided.


```{r}
mouse_325 <- exclude_anatomy(mouse_325, 
                        slice_ID = '1_2',
                        hemisphere = 'right',
                        channels = c("eyfp", "cfos", "colabel"),
                        clean = TRUE, 
                        exclude_right_regions = c("TRS", "LSX"),
                        exclude_layer_1 = TRUE, 
                        exclude_hemisphere = TRUE)


```


Let's do the same thing for the full slice, except we set exclude_hemisphere to FALSE since there is no contralateral hemisphere to exclude. Use both the `exclude_right_regions` and `exclude_left_regions` parameter to specify which regions to exclude from each hemisphere.


```{r}
mouse_325 <- exclude_anatomy(mouse_325, 
                        slice_ID = '1_1',
                        hemisphere = NULL,
                        channels = c("eyfp", "cfos", "colabel"),
                        clean = TRUE, 
                        exclude_right_regions = c("MOs", "MOp", "SSp-ul", "SSp-II", "ACAd1", "OT"),
                        exclude_left_regions = c("MOs1","MOs2/3", "MOs5", "MOp1", "MOp2/3", "MOp5", "ACAd2", "OT"),
                        exclude_layer_1 = TRUE, 
                        exclude_hemisphere = FALSE)

```



# 8 Normalize cell counts by region

Getting cell counts per region normalized by volume or area in mm^3^ or mm^2^ respectively.

## 8.1 Get slice volumes

In order to get cell counts per region in each mouse normalized by volume, the exact volume of each region in each slice needs to be calculated. This is accomplished with the function  `get_registered_volumes()`. The function will automatically calculate region volumes per hemisphere for 
each slice.


```{r}

# Calculate region volumes for the full slice
mouse_325 <- get_registered_volumes(mouse_325,
                                  slice_ID = "1_1",
                                  hemisphere = NULL)

# Calculate region volumes for the right-hemisphere slice
mouse_325 <- get_registered_volumes(mouse_325,
                                  slice_ID = "1_2",
                                  hemisphere = "right")


```



## 8.2 Get a combined cell data table 

The `get_cell_table()` function will aggregate all forward warped cell counts across all slices one data frame named `cell_table`. This can be especially useful for plotting purposes.

To save time, we will now pre-load mouse_325 containing multiple slice objects that have been registered, forward-warped, and cleaned, with registered volumes calculated.

```{r, eval = TRUE}
# Pre-load saved processed mouse  Modify the path to server V if necessary
load("V:/Michelle Jin/Sundowning/PipelineDemo/Mouse_example/mouse_325_pre-registered_forward_warped.RDATA")
```


```{r, eval=TRUE}

mouse_325 <- get_cell_table(mouse_325, channels = c("cfos", "eyfp", "colabel"))

```

You can access the cell table for each channel with the `$` operator (e.g. `mouse_325$cell_table$cfos`). We can use this aggregated dataset if we want to plot an interactive "glass brain" plot of all slices in the mouse.

```{r, eval=TRUE}
# Plot an interactive 3D plot of the cfos channel
SMART::glassbrain2(mouse_325$cell_table$cfos, jitter = FALSE)
```

This is a useful function to show an interactive representation of all the cells cells mapped in a single animal. 


## 8.3 Get normalized cell counts 


Once `get_registered_volumes()` has been run for all the slice objects within a mouse, and the forward warped counts have been combined using `get_cell_table()`, we can use the function `normalize_cell_counts()` to get normalized cell counts per volume (mm^3^) and per area (mm^2^). 

This information is stored as a named element in the mouse called `normalized_counts`. 


> Tip: The parameter `simplify_regions` will further collapse the normalized cell counts by certain keywords (e.g. "layer" or "stratum"). If a region name is detected to have one of these keywords, it will merge counts with its parent structure until there are no more keywords found. This reduces the overwhelming amount of substructures that we can compare and helps simplify our analysis.



```{r}
mouse_325 <- normalize_cell_counts(mouse_325, 
                                   combine_hemispheres = TRUE, 
                                   simplify_regions = TRUE)

print(mouse_325$normalized_counts)
```

## 8.4 Split hippocampal counts into Dorsal/Ventral counts (Optional)

Sometimes, we want to further subdivide the hippocampus into *dorsal* and *ventral* subregions. The `wholebrain` atlas plates are derived from the Allen Mouse Brain Atlas, which does not intrinsically have *dorsal* and *ventral* subdivisions for the hippocampus. Our current strategy is to use an AP coordinate cutoff, where hippocampal counts anterior to this cutoff are considered *dorsal* and posterior to this cutoff are considered *ventral*. 


```{r}


mouse_325 <- split_hipp_DV(mouse_325, AP_coord = -2.7, combine_hemispheres = TRUE, simplify_regions = TRUE, merge = FALSE)


print(mouse_325$hipp_DV_normalized_counts)

```

We can choose to overwrite the existing hippocampal data stored in the `cell_table` with the hippocampal data split dorsal and ventrally by setting the `merge` parameter to `TRUE`. I recommend this option, as all future analysis functions are designed to operate on this data. 

```{r}

mouse_325 <- split_hipp_DV(mouse_325, AP_coord = -2.7, combine_hemispheres = TRUE, simplify_regions = TRUE, merge = TRUE)
```





# 9 Aggregating mouse data

## 9.1 Initializing an experiment object


Once you've registered enough mice, you can begin adding them into an experiment object. Creating an experiment object is very similar to the way we created mouse and slice objects with one exception: there are certain experimental attributes that are meant to be autopopulated when we add a mouse object to it and should be left alone during object construction. 


For example, if multiple mice are added to an experiment object from three different `drug` conditions, the experiment object's attribute `drug_groups` will consist of the names of the three drugs given. Check the help page to see which experiment attributes are autogenerated. You'll see that the `experiment_name`, `experimenters`, and `output_path` parameters are the only ones that really need to be set manually. 


Additionally, know that adding mice to an experiment will keep only the processed neural mapping information, not the individual slice information. This is to ensure that unnecessary computer memory isn't being used during analysis. Therefore, any changes you want to make to cleaning up or modifying individual slice data should be done at the mouse object level. 



```{r}

sundowning <- experiment(experiment_name = "sundowning",
                         experimenters = c("HH", "AW", "MJ"),
                         output_path = "V:\\Michelle Jin\\Sundowning\\PipelineDemo\\Experiment_example")
 
sundowning <- add_mouse(sundowning, mouse_325)

```


Just like a mouse object, you can save an experiment object, with or without a timestamp.

```{r}
save_experiment(sundowning)
```


## 9.2 Combine cell counts 

Once you've added enough mice to perform an analysis, we want to aggregate all the data for each mouse together into one dataframe to perform analysis on. This is done with the `combine_norm_cell_counts()` function.  


`by` is a special parameter that will allow up to take advantage of the many mouse attributes that we have recorded during creation of a mouse object. It is a vector of the mouse attributes we will use to split our dataset into subgroups for comparison. In the example below, we use the `group` and `sex` mouse attributes to compare `AD` and `control` groups and the `male` and `female` mice. Sometimes, certain attributes like `drug` may not apply to your experiment, so `by` should only include the variables that you intend on using for group comparisons during analysis. 


For now we will load a saved example experiment object named `sundowning`. *Please note* that the data in this object is fabricated. It is simply mouse_325's data with added noise. This is because we are still currently in the process of generating all the segmentation data for Holly. The following analyses and plots will not be scientifically informative.


```{r, eval=TRUE}

load("V:\\Michelle Jin\\Sundowning\\PipelineDemo\\Experiment_example\\experiment_demo_saved.Rdata")

sundowning <- combine_norm_cell_counts(sundowning, by = c('group', 'sex'))

```


# 10 Quality checking your data

The quality of the segmentation data may depend on many factors including immunolabelling quality, the sectioning and mounting technique, and performance of the segmentation algorithm. We also want to check for "statistical quality" and ensure that there are enough mice per group within a single brain region to compare. There are a few of functions we can use to check the data which can optionally clean our experimental dataset.


## 10.1 Check for outlier counts 

The function `find_outlier_counts()` will first organize counts into analysis groups stratified based on the `by` parameter. Then the mean cell counts for each analysis group and their standard deviation are calculated. If any regions counts for a mouse exceed greater than `n_sd` (default = 2) for their analysis group, the region and mouse will be marked as an outlier. If `log = TRUE` then, the output is stored in a csv file with the file stem `region_count_outliers_[channel]`. This may be helpful to look back at the raw data and examine whether the segmentation algorithm is doing a good job around these regions for a given mouse. 


```{r}

#
sundowning <- find_outlier_counts(sundowning, by = c("group", "sex"), n_sd = 2, remove = FALSE, log = TRUE)

```

> Set `remove` to `TRUE` if you want to permanantly remove outlier regions from the experiment object


## 10.2 Check if there are enough mice per region

In an ideal world, we would have all the same regions count data across all the mice we process. In reality, a given brain region may only have counts from a few mice within in analysis group. This may be because it was excluded from certain mice due to poor quality or tears, or it was not in one of the imaged sections. Because of this, we may want to exclude any region counts in an analysis group that has too low of an N.

This is done with the `enough_mice_per_group()` function. The usage is similar to the function for finding outlier counts. When `log = TRUE`, the csv file `regions_below_N_thresh.csv` in saved in the experimental output folder.

```{r}
sundowning <- enough_mice_per_group(sundowning, by = c("group", "sex"), min_n = 5, remove = FALSE, log = TRUE)
```


# 11 Analysis and statistical functions

This section will cover functions that analyze our data. Plotting of the output from these functions will be covered in section 12. The data output from these functions will always be stored in the experiment object and depending on the function, it can be exported as a csv file to the experiment output folder. 


## 11.1 Get colabel percentages

The function `get_percent_colabel()` will calculate the percentage of colabelled cells over either the eyfp or the cfos channels. Only the common regions between the colabelled and the chosen reference channel will be used during the calculation.

There are also different ways to calculate the data. If the parameter `individual` is set to `TRUE`, then the counts for each individual mice are preserved.  If `individual` is set to `FALSE` then individual percentage colabelled cells are collapsed into an analysis group average. The `by` parameter is used to determine the analysis groups. There is an additional `roi` parameter which allows for calculating the percentage of colabelled cells for a specific brain region. Below are examples of different ways to use the function:


Calculate percent colabel for individual mice and save it to the output folder

```{r, eval=TRUE}
sundowning <- get_percent_colabel(sundowning, channel = "eyfp", save_table = TRUE, individual = TRUE)
```


Calculate percent colabel for mice grouped by experimental group and sex and save it to the output folder

```{r, eval=TRUE}
sundowning <- get_percent_colabel(sundowning, by = c("group", "sex"), channel = "eyfp", save_table = TRUE, individual = FALSE)
```

Calculate percent colabel for mice grouped by experimental group and sex for just the dorsal DG and save it to the output folder

```{r}
sundowning <- get_percent_colabel(sundowning, by = c("group", "sex"), channel = "eyfp", roi = "dDG", save_table = TRUE, individual = FALSE)
```


## 11.2 Get pairwise region correlations

The `get_correlations()` function is used to calculate pairwise Pearson correlations across all regions within an analysis group. This can be done for each channel specified. Later we will use this to generate a correlation heatmap. There is also an option to adjust for p-values using different multiple comparisons methods using the and a  user-specified alpha value can also be applied to threshold significance. Check the function's help page for more details.

Like previous functions, there is a `by` parameter so the function can determine which variables to used to stratify your data into groups, and because each heat map is calculated for one set of grouping variable values, these need to be specified with the `values` parameter. The following example will generated a correlation matrix for cfos and eyfp cell counts from only female AD mice. The Bejamini-Hochberg method will be used to adjust for multiple comparisons and we will use an alpha threshold of 0.05.


```{r, eval=TRUE}
# Get region pairwise correlation coefficients for female ADs
sundowning <- get_correlations(sundowning,
                               by = c("sex", "group"), 
                               values = c("female", "AD"), 
                               channels = c("cfos", "eyfp", "colabel"),  
                               p_adjust_method = "BH", 
                               alpha = 0.05)
```


Because the values of the grouping variables identify a unique analysis group, they are used to name the stored results in the experiment separated, with values separated by an "_". For example, the analysis above has stored the results as a list under the name `female_AD.` In the next function, we will refer to this as the correlation list name.


```{r}
sundowning$correlation_list$female_AD
```


Let do the same thing for the female controls.

```{r, eval=TRUE}
# Get region pairwise correlation coefficients for female controls
sundowning <- get_correlations(sundowning,
                               by = c("sex", "group"), 
                               values = c("female", "control"), 
                               channels = c("cfos", "eyfp", "colabel"),  
                               p_adjust_method = "BH", 
                               alpha = 0.05)
```


## 11.3 Permute pairwise region correlation difference between groups


We can compare the difference between pairwise region correlations between two different analysis functions using a permutation analysis. For this we'll use the function `correlation_diff_permutation()`. This function requires you to have run `get_correlations()` for each analysis group that you want to compare prior to using it. 

It also allows you to specify the number of shuffles, the random seed number (for figure replication), as well as a multiple comparisons adjustment (the previous adjustment from `get_correlations()` is not redundantly applied).


In the example below, we will compare the `female_AD` analysis group with the `female_control` group.

```{r,eval=TRUE}

sundowning <- correlation_diff_permutation(sundowning,
                                           correlation_list_name_1 = "female_AD",
                                           correlation_list_name_2 = "female_control",
                                           n_shuffles = 1000,
                                           seed = 5,
                                           alpha = 0.01,
                                           p_adjust_method = "fdr",
                                           channels = c("cfos", "eyfp", "colabel"))



```

The results of the analysis are stored withing the experiment object in list called `permutation_p_matrix` 

```{r}
sundowning$permutation_p_matrix$
```



## 11.4 Create region networks with summary stats

Now we can move on to the process of automatically creating networks in R using the `create_networks()` function. This function is also contingent on running `get_correlations()` first because these networks are constructed on the correlation coefficents. There a some customizable features such as setting the edge color (taken as a hexadecimal string) if you have pre-chosen colors for your analysis groups. In the example below, we will just take the default edge color.

```{r, eval=TRUE}
# Create a network for the female AD group
sundowning <- create_networks(sundowning, 
                              correlation_list_name = "female_AD",
                              alpha = 0.1)


# Create a network for the female control group
sundowning <- create_networks(sundowning, 
                              correlation_list_name = "female_control",
                              alpha = 0.1)

```

After running this function, you may be wondering where the network is. Rest assured that a network object for each channel per analysis group was created using `tidygraph` and they have been stored in our experiment object. We can access the data with `sundowning$networks$network_name` where network_name is identical to the `correlation_list_name` used to generate the network. 

Now there are some network summary statistics that we can calculate using our network object. We will calculate them using the function
`summarise_networks()`. This function is designed to summarise the stats of multiple networks at once if they are supplied to the parameter 
`network_names.`

The additional parameters `save_stats`, `save_degree_distribution`, and `save_betweenness_distribution` are used to save the general network summary statistics, the network degree distributions, and the betweenness distributions as csv files in the experiment object folder. This is handy if you would prefer to graph these values in Prism instead of R.

```{r, eval=TRUE}

sundowning <- summarise_networks(sundowning, 
                                 network_names = c("female_AD", "female_control"),
                                 save_stats = TRUE,
                                 save_degree_distribution = TRUE,
                                 save_betweenness_distribution = TRUE)


```


# 12 Plotting functions

Hooray! All of this hard work will allow use to generate some beautiful and hopefully informative plots (for real datasets). 


## 12.1 Plot percentage colabel

The first thing we may want to plot is 






## 12.2 Plot heatmaps of region correlations

```{r}
plot_correlation_heatmaps(sundowning, 
                          correlation_list_name = "female_AD",
                          print_plot = TRUE,
                          save_plot = FALSE,
                          height = 10,
                          width = 10)
```



## 12.3 Plot permutation analysis (volcano plots & parallel coordinate plots)



```{r}
volcano_plot(sundowning,
             permutation_comparison = "female_AD_vs_female_control",
             channel = c("cfos", "eyfp", "colabel"),
             print_plot = TRUE,
             alpha = 0.001,
             save_plot = FALSE)
```



```{r}
parallel_coordinate_plot(sundowning,
                         permutation_comparison = "female_AD_vs_female_control",
                         print_plot = TRUE,
                         save_plot = FALSE,
                         )
```

## 12.4 Plot network analysis and network summary statistics



```{r}
plot_networks(sundowning,
              network_name = "female_AD",
              title = "Female AD",
              channels = "cfos",
              # edge_color = c("firebrick", "firebrick", "firebrick"),
              height = 15,
              width= 15,
              print_plot = TRUE,
              save_plot = FALSE
              )
```






# 13 Scripting

Scripting in R is more advanced topic, however it can save a ton of time in automating tasks that are repetitive, such as object creation or looping registrations across all the slices. The following sections will cover different scripts that can help automate these tasks.


## 13.1 Creating mouse and slice objects using a script

We can make our lives a little easier (and more organized) by reading a csv file that initially contains all of the mouse and slice attribute 
data prior to creating the objects.

```{r, eval=FALSE}
# This batch script:
# Reads the parameters from a csv file
# Auto creates the mouse and slice objects
# Automatically saves the mouse objects in designated mouse output folder


# Dependencies:
library(SMARTR)
library(tidyverse)
library(magrittr)
library(stringdist)


# Read the dataframe
csv_path <-"V:\\Michelle Jin\\Sundowning\\MasterSliceMouseObject_9.21.2021.csv"

#_____________________ autocreate mouse objects _______________________________

df <- read_csv(csv_path, na = c("", "NA"))


# Isolate columns with mouse metadata and slice metadata
mouse_cols <- names(df[1:(which(names(df) == "slice") - 2)])
slice_cols <- names(df[which(names(df) == "slice"):length(df)])

# select mouse column data and collapse redundant information
mice_info <- df %>% dplyr::select(all_of(mouse_cols)) %>% distinct()

# Autocreate each mouse object and save in the mouse directory as it's output folder
for (m in 1:length(mice_info$mouse)){

  my_mouse <- mouse(mouse_ID = mice_info$mouse[m],
                    sex = mice_info$sex[m],
                    age = mice_info$age[m],
                    cre_genotype = mice_info$cre_genotype[m],
                    reporter = mice_info$reporter[m],
                    strain = mice_info$strain[m],
                    experiment = mice_info$experiment[m],
                    group = mice_info$group[m],
                    drug = mice_info$drug[m],
                    cohort = mice_info$cohort[m],
                    output_path = mice_info$mouse_path[m])


  # Save the mouse, then remove it from the environment
  assign(paste0("mouse_", mice_info$mouse[m]), my_mouse)
  eval((parse(text=paste0("save_mouse(mouse_", mice_info$mouse[m],")"))))
  eval((parse(text=paste0("rm(mouse_", mice_info$mouse[m],")"))))

}

#_____________________ autocreate slice objects _______________________________

# For each new slice
# Detect if the mouse has changed
# load the mouse
# populate the slice info
# Add it to the mouse
# check if the next slice belongs to the same mouse
# if not, save the mouse and delete it from the environment
# move onto next mouse

change_mouse <- TRUE
for (s in 1:length(df$slice)){

  # Check if this is a skipped slice. Skip current slice if it is.
  if (isTRUE(df$`Skip (y)`[s] == "y")) {
     next
  }

  if (change_mouse && s != 1){
    # search, save, then remove current mouse
    eval((parse(text=paste0("save_mouse(", cur_mouse, ")"))))
    eval(parse(text=paste0("rm(", cur_mouse, ")")))

    # Load the new mouse
    load(file.path(df$mouse_path[s], paste0("mouse_", df$mouse[s], ".RDATA")))
    cur_mouse <- ls()[amatch("mouse_", ls(), maxDist = 4)]
    change_mouse <- FALSE
  } else if (s == 1) {
    # Load the new mouse
    load(file.path(df$mouse_path[s], paste0("mouse_", df$mouse[s], ".RDATA")))

    # current mouse
    cur_mouse <- ls()[amatch("mouse_", ls(), maxDist = 4)]
    change_mouse <- FALSE
  }


  # Fill the hemisphere information
  if (is.na(df$hemisphere[s])){
    hemi <- NULL
  } else {
    hemi <- df$hemisphere[s]
  }

  # Populate the slice information
  my_slice <- slice(slice_ID = df$slice[s],
                    hemisphere = hemi,
                    coordinate = df$`atlas coordinate`[s],
                    registration_path = df$registration_path[s])

  eval(parse(text = paste0(cur_mouse, "<- add_slice(", cur_mouse, ", my_slice)")))
  print(  eval(parse(text = paste0(cur_mouse))))

  # get current mouse number
  cur_mouse_no <- str_split(cur_mouse, "_") %>% unlist()
  cur_mouse_no <- cur_mouse_no[-1] %>% as.double()

  if (s == length(df$mouse)){
    # Save if its the last slice in a mouse
    eval((parse(text=paste0("save_mouse(", cur_mouse, ")"))))
  } else if (df$mouse[s + 1] != cur_mouse_no){
    change_mouse = TRUE
  }

}



```


## 13.2 Looping through the registration process


## 13.3 Looping through segmentation and filtering

## 13.4 Looping through cleaning up cell counts

## 13.4 Looping through forward warping



## 13.5 


</div>
</font> 




# SCRATCH SCRATCH SCRATCH


```{r}
sundowning <- experiment(experiment_name = "sundowning",
                         experimenters = c("HH", "AW", "MJ"),
                         output_path = "V:\\Michelle Jin\\Sundowning\\PipelineDemo\\Experiment_example")
 

```

```{r}

for (k in 1:10){


# k <- 3

  load(paste0("V:/Michelle Jin/Sundowning/PipelineDemo/325/false_control_mouse_", k, ".RDATA"))
  print(m$normalized_counts$cfos[m$normalized_counts$cfos$acronym== "AAA",])

  sundowning <- add_mouse(sundowning, m, replace = TRUE)
}





```


```{r}

attr(sundowning, "info")$output_path <- "V:\\Michelle Jin\\Sundowning\\PipelineDemo\\Experiment_example"

save_experiment(e, timestamp = TRUE)
```



```{r}
e <- sundowning

e <- combine_norm_cell_counts(e)

e_info <- attr(e, "info")
```


Choose a random number of regions to delete from each test mouse's normalized cells count
```{r}
for (channel in e_info$channels){
# 
#   e$combined_normalized_counts[[channel]] <- e$combined_normalized_counts[[channel]][e$combined_normalized_counts[[channel]]$mouse_ID != "325",]
  nrows <- e$combined_normalized_counts[[channel]]$mouse_ID %>% length() 
  e$combined_normalized_counts[[channel]] <- e$combined_normalized_counts[[channel]] %>% sample_frac(size = .9, replace = FALSE)

}
```



```{r}
e <- enough_mice_per_group(e, by = c("group", "sex"), min_n = 4)
```

```{r}

save_experiment(e, timestamp = TRUE)

```
















## Create a fake experiment

```{r}

# Create experiment
sundowning <- experiment(experiment_name = "sundowning",
                         experimenters = c("HH", "AW", "RC", "MJ"),
                         output_path = "V:\\Michelle Jin\\Sundowning\\PipelineDemo\\Experiment_example")

# List the files
fake_mouse_folder <- "V:\\Michelle Jin\\Sundowning\\PipelineDemo\\Mouse_example\\fake_mice"
files <- list.files(fake_mouse_folder)
  

# Loop through files
for (n in 1:length(files)){
  
  # Load the files
  load(file.path(fake_mouse_folder, files[[n]]))
  
  # Fix the mouse name
  attr(m,"info")$mouse_ID <- paste0("false_mouse_", n)
  
  # Add the mouse to the Sundowning experiment
  sundowning <- add_mouse(sundowning, m)
  
}  
  

```






